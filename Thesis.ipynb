{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3f7349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import pytz\n",
    "from scipy.stats import skew, kurtosis, t, shapiro\n",
    "from statistics import variance\n",
    "from datetime import *\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import blpapi\n",
    "from binance.client import Client\n",
    "from binance.spot import Spot\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from xbbg import blp\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from tabulate import tabulate\n",
    "from texttable import Texttable\n",
    "import latextable\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73513058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following packages are installed:\n",
    "pip install --index-url=https://bcms.bloomberg.com/pip/simple blpapi\n",
    "pip install xbbg\n",
    "%pip install blp\n",
    "# I need to import the request library for APIs\n",
    "%pip install requests\n",
    "pip install vaderSentiment\n",
    "%pip install latextable\n",
    "%pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61e2f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File with all the raw values of the S&P 500\n",
    "data_SP500 = pd.read_excel('S&P 500 daily HFT data.xlsx')\n",
    "# Pre-processed file with the final and ready to use values of the S&P 500\n",
    "data_SP500 = pd.read_csv('S&P500 full df.csv', index_col='Timestamp')\n",
    "data_SP500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487908ec",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Collection of the data and pre-processing</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff45ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S&P 500 data collection \n",
    "\n",
    "# dropping the columns without values \n",
    "data_SP500 = data_SP500.drop(columns=['Unnamed: 0', 'Unnamed: 5', 'Unnamed: 10'])\n",
    "\n",
    "# creating 3 different df for the differet columns inside the excel file and then I will concatenate them \n",
    "# to have all the high frequency data of the S&P 500 from 30 september 2021\n",
    "data_SP500_1 = data_SP500.iloc[:, 0:3].dropna(axis=0)\n",
    "data_SP500_2 = data_SP500.iloc[:, 4:7].dropna(axis=0)\n",
    "data_SP500_3 = data_SP500.iloc[:, 8:11].dropna(axis=0)\n",
    "\n",
    "# renaming the columns of the df to concatenate them \n",
    "data_SP500_1 = data_SP500_1.rename(columns ={'Unnamed: 1':'Date', 'Unnamed: 2':'type', 'Unnamed: 3':'price', 'Unnamed: 4':'volume'})\n",
    "data_SP500_2 = data_SP500_2.rename(columns ={'Unnamed: 6':'Date', 'Unnamed: 7':'type', 'Unnamed: 8':'price', 'Unnamed: 9':'volume'})\n",
    "data_SP500_3 = data_SP500_3.rename(columns ={'Unnamed: 11':'Date', 'Unnamed: 12':'type', 'Unnamed: 13':'price', 'Unnamed: 14':'volume'})\n",
    "\n",
    "\n",
    "# concatenating the three df so that I have just one df with all the prices in sequence by time\n",
    "data_SP500 = pd.concat([data_SP500_1, data_SP500_2, data_SP500_3])\n",
    "\n",
    "# making all the dates in datetime format with year, month, day...\n",
    "data_SP500.Date[data_SP500.Date.apply(lambda x: isinstance(x, datetime))] = data_SP500.Date[data_SP500.Date.apply(lambda x: isinstance(x, datetime))].apply(lambda x: datetime.strftime(x, '%Y-%d-%m %I:%M:%S %p'))\n",
    "data_SP500.Date = pd.to_datetime(data_SP500.Date)\n",
    "\n",
    "# computing also the date in the timestamp - epoch format\n",
    "data_SP500.Timestamp = ((data_SP500.Date - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's'))\n",
    "data_SP500.Timestamp = data_SP500.Timestamp.apply(lambda x: '%.0f' % x)\n",
    "# setting the timestamp as index for the df\n",
    "data_SP500 = data_SP500.set_index(data_SP500.Timestamp)\n",
    "data_SP500 = data_SP500.drop(columns=['Timestamp'])\n",
    "data_SP500\n",
    "\n",
    "# converting the 12h time formats to 24h without \"PM\"\n",
    "datetime.strptime(data_SP500['Unnamed: 1'][0][:], '%Y,%m,%d,%I:%M:%S %p')\n",
    "\n",
    "data_SP500 = data_SP500.reset_index()\n",
    "data_SP500.Date.datetime.values.astype(np.int64) // 10 ** 9\n",
    "data_SP500.Date.to_timestamp(how='s')\n",
    "\n",
    "data_SP500.Timestamp = ((data_SP500.Date - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's'))\n",
    "data_SP500.Timestamp = data_SP500.Timestamp.apply(lambda x: '%.0f' % x)\n",
    "data_SP500 = data_SP500.set_index(data_SP500.Timestamp)\n",
    "data_SP500 = data_SP500.drop(columns=['Timestamp'])\n",
    "data_SP500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the log returns for all the prices available \n",
    "data_SP500['ret'] = np.log(data_SP500.price/data_SP500.price.shift())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4998822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the trading activity details (descriptive statistics) of the 24-02 (day in which the war started)\n",
    "# data_SP500[data_SP500['Date'].str.contains('2022-02-24 15:45:0')].describe()\n",
    "\n",
    "# crating a df for the war analysis on the S&P 500\n",
    "war = data_SP500.loc[1645654792:1645717590] # last 10 trades of the previous day for the closing price and 16 mins \n",
    "# of the current day in order to have 1000 rows of data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8baa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at 20 there was the FOMC press conference - Powell spoke \n",
    "(data_SP500[data_SP500['Date'].str.contains('2021-12-15 20:00')])\n",
    "\n",
    "tapering = data_SP500.loc[1639598000:1639599000] # 5 minutes before and 10 mins after in order to have 1000 rows of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f3c666",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# live announcement (From Powell) that possible rate hike will come in March (1+ month from the news)\n",
    "(data_SP500[data_SP500['Date'].str.contains('2022-01-26 20:09')])\n",
    "\n",
    "rates_hike = data_SP500.loc[1643226800:1643227800] # 7 minutes before and 10 mins after in order to have 1000 rows of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d0ec65",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Abnormal Returns and T-Test</span> \n",
    "\n",
    "## Computing AR, AAR and CAAR. the p-values for the AAR every 10 seconds after the event are computed according to the methodology used by Brown and Warner (1980)\n",
    "\n",
    "\n",
    "\n",
    "<span style=\"color:green\">I also check for normality in the distribution of AR. When the p value of AAR is lower that 0.05, then I can reject the null hypothesis that the news have no impact on the returns</span> \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca9ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using 5 mins (300) as event window and 120 mins=2 hours(7200)\n",
    "\n",
    "\n",
    "# defining a function to compute the abnormal returns over 10 seconds interval\n",
    "def abn_returns_10s_new(df):\n",
    "    # I will always compute the abnormal returns for the 5 minutes after the news and use as estimation window the \n",
    "    # 10 mins before. So the df will have exactly 300+7200=7500 rows and the position 7200 will be the news\n",
    "    avg_ret = df.ret.iloc[0:7200].mean()\n",
    "    ar = pd.DataFrame(columns=['Date', 'AAR', 'CAR', 'CAAR'], index = range(0,30))  # index = df.Date.iloc[600:]\n",
    "    n = 7200\n",
    "    ar['significance'] = ''\n",
    "    ar['significance_CAR'] = ''\n",
    "    ar['significance_AR'] = ''\n",
    "    df['AR'] = df.ret - avg_ret\n",
    "    \n",
    "    # trying to compute the CAR in a meaningful way \n",
    "    df['CAR'] = 0\n",
    "    df.CAR.iloc[7200:7500] = df.AR.iloc[7200:7500].cumsum()\n",
    "    \n",
    "    df = df.fillna(0)\n",
    "    AAR = df.AR.iloc[0:7200].mean()\n",
    "    # considering the return every 10 seconds \n",
    "    for i in range(0, 30):\n",
    "        ar.Date.iloc[i] = df.Date.iloc[n]\n",
    "    # AR is: observed ret-average\n",
    "    # I compute the Cumulative Abnormal Return for every 10 seconds\n",
    "        ar.CAR.iloc[i] = df.CAR.iloc[n:n+10].mean()\n",
    " \n",
    "    # I compute the Average Abnormal Return for every 10 seconds\n",
    "        ar.AAR.iloc[i] = df.AR.iloc[n:n+10].mean()\n",
    "        \n",
    "        n = n +10\n",
    "    # I compute the Cumulative Average Abnormal Return for every 10 seconds, summing the AAR    \n",
    "    ar.CAAR = ar.AAR.cumsum()\n",
    "    \n",
    "   \n",
    "    \n",
    "    # formula to compute the p-value\n",
    "    # using the shapiro test for normality:\n",
    "    shapiro_test_norm = shapiro(ar.CAR).pvalue\n",
    "#     using the Jarque-Bera test for normality:\n",
    "#     test_for_norm = (300/6)*((skew(df.AR.iloc[7200:7500].cumsum())**2)*(((kurtosis(df.AR.iloc[7200:7500].cumsum())-3)**2)/4))\n",
    "#     test_for_norm = (len(ar.CAR)/6)*((skew(ar.CAR)**2)*(((kurtosis(ar.CAR)-3)**2)/4))\n",
    "#     print('J-B Test for normality: ' + str(test_for_norm), 'Shapiro Test for normality: ' + str(shapiro_test_norm))\n",
    "\n",
    "    print('Shapiro Test for normality: ' + str(shapiro_test_norm))\n",
    "    \n",
    "    if shapiro_test_norm > 0.05:  #  test_for_norm < 0.05 or \n",
    "        \n",
    "        \n",
    "        ar['t_test_CAR'] = ar.CAR/np.std(ar.CAR)\n",
    "        ar['t_test_AR'] = ar.AAR/np.std(ar.AAR)\n",
    "        \n",
    "        ar.significance_CAR.loc[ar['t_test_CAR'] >= 1.65] = '*'\n",
    "        ar.significance_CAR.loc[ar['t_test_CAR'] <= -1.65] = '*'\n",
    "        ar.significance_CAR.loc[ar['t_test_CAR'] >= 1.96] = '**'\n",
    "        ar.significance_CAR.loc[ar['t_test_CAR'] <= -1.96] = '**'\n",
    "        ar.significance_CAR.loc[ar['t_test_CAR'] >= 2.58] = '***'\n",
    "        ar.significance_CAR.loc[ar['t_test_CAR'] <= -2.58] = '***'\n",
    "        \n",
    "        ar.significance_AR.loc[ar['t_test_AR'] >= 1.65] = '*'\n",
    "        ar.significance_AR.loc[ar['t_test_AR'] <= -1.65] = '*'\n",
    "        ar.significance_AR.loc[ar['t_test_AR'] >= 1.96] = '**'\n",
    "        ar.significance_AR.loc[ar['t_test_AR'] <= -1.96] = '**'\n",
    "        ar.significance_AR.loc[ar['t_test_AR'] >= 2.58] = '***'\n",
    "        ar.significance_AR.loc[ar['t_test_AR'] <= -2.58] = '***'\n",
    "        print('Normal distributed CAR')\n",
    "        \n",
    "        \n",
    "    # sigma of AR in the estimation window\n",
    "    sigma = sqrt(sum((df.AR.iloc[0:7200]-AAR)**2)/7200)\n",
    "\n",
    "    # computing t-statistics for AAR and CAAR\n",
    "    ar['t_test_AAR'] = ar.AAR/sigma\n",
    "    ar['t_test_CAAR'] = ar.CAAR/((301**0.5)*sigma)\n",
    "\n",
    "    ar['significance_CAAR'] = ''\n",
    "    ar.significance_CAAR.loc[ar['t_test_CAAR'] >= 1.65] = '*'\n",
    "    ar.significance_CAAR.loc[ar['t_test_CAAR'] <= -1.65] = '*'\n",
    "    ar.significance_CAAR.loc[ar['t_test_CAAR'] >= 1.96] = '**'\n",
    "    ar.significance_CAAR.loc[ar['t_test_CAAR'] <= -1.96] = '**'\n",
    "    ar.significance_CAAR.loc[ar['t_test_CAAR'] >= 2.58] = '***'\n",
    "    ar.significance_CAAR.loc[ar['t_test_CAAR'] <= -2.58] = '***'\n",
    "    \n",
    "    \n",
    "    ar.significance.loc[ar['t_test_AAR'] >= 1.65] = '*'\n",
    "    ar.significance.loc[ar['t_test_AAR'] <= -1.65] = '*'\n",
    "    ar.significance.loc[ar['t_test_AAR'] >= 1.96] = '**'\n",
    "    ar.significance.loc[ar['t_test_AAR'] <= -1.96] = '**'\n",
    "    ar.significance.loc[ar['t_test_AAR'] >= 2.58] = '***'\n",
    "    ar.significance.loc[ar['t_test_AAR'] <= -2.58] = '***'\n",
    "    # setting the degrees of freedom: in the reference paper are 120 (days of the estimation window - 1)\n",
    "    # so here are 7200 - 1 = 7199\n",
    "    dof = 7199\n",
    "    for i in range(0, len(ar.t_test_AAR)):\n",
    "        ar.t_test_AAR.iloc[i] = 2*(1 - t.cdf(abs(ar.t_test_AAR.iloc[i]), dof))\n",
    "    \n",
    "    ar = ar.set_index('Date')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # creating a df in which i compute the AR over the first minute to do the analysis with high frequency data\n",
    "    df_abn_ret = pd.DataFrame()\n",
    "    df_abn_ret['AR'] = df.AR.iloc[7260:]\n",
    "    df_abn_ret['t_test_AR'] = (df.AR.iloc[7260:]/np.std(df.AR.iloc[0:7260]))\n",
    "    df_abn_ret['CAR'] = df_abn_ret.AR.cumsum()\n",
    "    # computing the significance level:\n",
    "    df_abn_ret['signif'] = 0.0\n",
    "    df_abn_ret.signif.loc[df_abn_ret['t_test_AR'] >= 1.65] = '*'\n",
    "    df_abn_ret.signif.loc[df_abn_ret['t_test_AR'] <= -1.65] = '*'\n",
    "    df_abn_ret.signif.loc[df_abn_ret['t_test_AR'] >= 1.96] = '**'\n",
    "    df_abn_ret.signif.loc[df_abn_ret['t_test_AR'] <= -1.96] = '**'\n",
    "    df_abn_ret.signif.loc[df_abn_ret['t_test_AR'] >= 2.58] = '***'\n",
    "    df_abn_ret.signif.loc[df_abn_ret['t_test_AR'] <= -2.58] = '***'\n",
    "    avg_car_1 = df_abn_ret.CAR.iloc[0:60].mean()\n",
    "    avg_car_10 = df_abn_ret.CAR.iloc[0:600].mean()\n",
    "\n",
    "    print(df_abn_ret.iloc[0:60], avg_car_1, avg_car_10)\n",
    "    \n",
    "    return df_abn_ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9918b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the funciton to obtain the AR, CAR, CAAR, AAR and the significance of the values\n",
    "# taking 7200 sec before and 700 after, because I need 600 fro the CAR mean, but the event window is 300\n",
    "\n",
    "\n",
    "# 26 jan - interest rates hike (with 1 data each 10 seconds (30 rows result))\n",
    "abnormal_ret_rates_hike = abn_returns_10s_new(data_SP500.loc[1643219940:1643227840]) \n",
    "\n",
    "# 15 dec - tapering (with 1 data each 10 seconds (30 rows result))\n",
    "abnormal_ret_tappering = abn_returns_10s_new(data_SP500.loc[1639591140:1639599040]) \n",
    "# with all the seconds (300 rows result)\n",
    "\n",
    "# 24 feb - war \n",
    "# I first need to trdop the wrong trades after 22 (because the market is closed and returns are all close to 0)\n",
    "clean_23_feb = data_SP500.loc[1645646340:1645717240]\n",
    "clean_23_feb.drop(index= range(1645653601,1645654801), inplace=True)\n",
    "clean_23_feb.drop(index= [1645657418], inplace=True) \n",
    "# (with 1 data each 10 seconds (30 rows result))\n",
    "abnormal_ret_war = abn_returns_10s_new(clean_23_feb)\n",
    "# with all the seconds (300 rows result)\n",
    "\n",
    "\n",
    "\n",
    "# and also for CRIX:\n",
    "abnormal_ret_tappering_1 = abn_returns_10s_new(tapering_1.loc[1639583940:1639594440])\n",
    "abnormal_ret_rates_hike_1 = abn_returns_10s_new(rates_hike_1.loc[1643216340:1643223840]) \n",
    "abnormal_ret_war_1 = abn_returns_10s_new(war_1.loc[1645661940:1645669440])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9db6743",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Cumulative returns in 15 mins interval</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making small df for each news in order to plot them easily\n",
    "# for interest rtes\n",
    "plot_rate = data_SP500.loc[1643226600:1643228400]\n",
    "plot_rate['time_event'] = (plot_rate.index-1643227200)/60\n",
    "plot_rate = plot_rate.set_index('time_event')\n",
    "\n",
    "# for tapering\n",
    "plot_tap = data_SP500.loc[1639597800:1639599600]\n",
    "plot_tap['time_event'] = (plot_tap.index-1639598400)/60\n",
    "plot_tap = plot_tap.set_index('time_event')\n",
    "\n",
    "# for war\n",
    "clean_23_feb = data_SP500.loc[1645653001:1645717801] \n",
    "clean_23_feb.drop(index= range(1645653601,1645654801), inplace=True)\n",
    "clean_23_feb.drop(index= [1645657418], inplace=True) \n",
    "clean_23_feb = clean_23_feb.reset_index()\n",
    "clean_23_feb = clean_23_feb.set_index('Date')\n",
    "plot_war = clean_23_feb\n",
    "ed = plot_war.loc['2022-02-24 15:30:01'].Timestamp\n",
    "plot_war['time_event'] = (plot_war.Timestamp-ed)/60\n",
    "plot_war.loc['2022-02-23 21:50:00':'2022-02-23 22:00:00'].time_event = plot_war.loc['2022-02-23 21:50:00':'2022-02-23 22:00:00'].time_event + 1050\n",
    "plot_war = plot_war.set_index('time_event')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f3586",
   "metadata": {},
   "source": [
    "Seems to be verified what stated in one paper: \"***Compared to previous research, we find that prices respond more quickly: a positive return in the first two minutes, with a partial reversal in the third minute.***\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92d309a",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Crypto analysis</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611e2c15",
   "metadata": {},
   "source": [
    "### the cryptos I will consider are the one in the CRIX on the date of 10-05-2022 and they are:\n",
    "### BTC, ETH, BNB, LUNA, SOL, XRP, ADA, DOT, AVAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e22160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create DF with the data collected and create the index for Cryptos - CRIX:\n",
    "def pre_processing_CC(c_1, c_2, c_3, c_4, c_5, c_6, c_7, c_8, c_9, start_time, end_time):\n",
    "    global new_df0\n",
    "    \n",
    "    cc = [c_1, c_2, c_3, c_4, c_5, c_6, c_7, c_8, c_9]\n",
    "    \n",
    "    for i in range(0, 9):\n",
    "        cc_use = cc[i]\n",
    "        response = requests.get('https://api.binance.com/api/v3/aggTrades?symbol=%(cc_use)s&startTime=%(start_time)s&endTime=%(end_time)s' %locals())\n",
    "        df = response.json()\n",
    "\n",
    "    # list with weights\n",
    "        weights = [0.5562, 0.2635, 0.0481, 0.0263, 0.0266, 0.0253, 0.0242, 0.0154, 0.0144] \n",
    "        \n",
    "        data = pd.DataFrame(df)\n",
    "        data = data.drop(columns=['a', 'm', 'M', 'f', 'l', 'q'])\n",
    "        data = data.rename(columns={'T': 'Timestamp', 'p':'price'}) #, 'q':'quantity'\n",
    "        data.Timestamp = data.Timestamp.astype(str)\n",
    "        data.Timestamp = data.Timestamp.str.slice(stop=-3)\n",
    "        data.Timestamp = data.Timestamp.astype(int)\n",
    "        data.price = data.price.astype(float)\n",
    "        data = data.groupby(by=['Timestamp']).mean()\n",
    "        data['Date'] =  pd.to_datetime(data.index, unit='s')\n",
    "        \n",
    "        \n",
    "        if i == 0:\n",
    "            globals()['new_df'+str(i)] = pd.DataFrame(df)\n",
    "            # dropping the columns that are not needed in this phase:\n",
    "            globals()['new_df'+str(i)] = globals()['new_df'+str(i)].drop(columns=['a', 'm', 'M', 'f', 'l', 'q'])\n",
    "            globals()['new_df'+str(i)] = globals()['new_df'+str(i)].rename(columns={'T': 'Timestamp', 'p':'price'}) # , 'q':'quantity'\n",
    "            # to consider just the timestamp till the seconds ===> is better for the index creation because instead is difficut\n",
    "            # to get the same timestamp and create prices. In this way I can simply create prices every second\n",
    "            globals()['new_df'+str(i)].Timestamp = globals()['new_df'+str(i)].Timestamp.astype(str)\n",
    "            globals()['new_df'+str(i)].Timestamp =  globals()['new_df'+str(i)].Timestamp.str.slice(stop=-3)\n",
    "            # adding a column with the date\n",
    "            globals()['new_df'+str(i)]['Date'] =  pd.to_datetime(globals()['new_df'+str(i)].Timestamp, unit='s')\n",
    "            \n",
    "\n",
    "            # making the price, Timestamp and the quantity floats\n",
    "            globals()['new_df'+str(i)].price = globals()['new_df'+str(i)].price.astype(float)\n",
    "\n",
    "\n",
    "            globals()['new_df'+str(i)].Timestamp = globals()['new_df'+str(i)].Timestamp.astype(int)\n",
    "\n",
    "            # grouping by timestamp and taking the average (just price needed in this analysis)\n",
    "            globals()['new_df'+str(i)] = globals()['new_df'+str(i)].groupby(by=['Date']).mean()\n",
    "            globals()['new_df'+str(i)].index = globals()['new_df'+str(i)].index #.tz_localize('Europe/Berlin')\n",
    "\n",
    "            # setting timestamp as index\n",
    "            globals()['new_df'+str(i)] = globals()['new_df'+str(i)].reset_index()\n",
    "            globals()['new_df'+str(i)] = globals()['new_df'+str(i)].set_index('Timestamp')\n",
    "\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # get the list of timestamps of both the dfs: \n",
    "            ind_0 = list(new_df0.index)\n",
    "            ind_1 = list(data.index)\n",
    "            \n",
    "            # make the difference betweeen the lists and create a new one with just the common ones\n",
    "            diff = list(set(ind_1) - set(ind_0))    # new dates to add to the starting df\n",
    "            common = list(set(ind_1) - set(diff))    # common dates to both the df\n",
    "            missing = list(set(ind_0) - set(ind_1))     # dates that are missing in the new df but are in the reference one\n",
    "    \n",
    "            # add the missing rows to the STARTING dataframe \n",
    "            to_add = pd.DataFrame(data.loc[diff])\n",
    "            to_add.price = 0\n",
    "            \n",
    "            # adding the missing rows to the starting DF in order to have them all complete with all the trades\n",
    "            new_df0 = new_df0.append(to_add)\n",
    "\n",
    "            \n",
    "            # fixing the referece df: new_df0:\n",
    "            # sort the df by timestamp\n",
    "            new_df0 = new_df0.reset_index()\n",
    "            new_df0 = new_df0.sort_values(by='Timestamp')\n",
    "            \n",
    "            # setting again Timestamp as index\n",
    "            new_df0 = new_df0.set_index('Timestamp')\n",
    "                \n",
    "                \n",
    "            # creating the new df\n",
    "            globals()['new_df'+str(i)] = new_df0\n",
    "\n",
    "            # add the prices to the df of interest with the whole list of dates        \n",
    "            globals()['new_df'+str(i)].loc[common] = data.loc[common]  # data.index.isin(common)\n",
    "            \n",
    "            # I also have to change the values of the missing one to NaN in order to then use the previous price\n",
    "            globals()['new_df'+str(i)].price.loc[missing] = 0  \n",
    "            \n",
    "            \n",
    "            # change NaN with price in the previous day\n",
    "            for k in range(0, len(globals()['new_df'+str(i)])):\n",
    "                if globals()['new_df'+str(i)].price.iloc[k] == 0:\n",
    "                    globals()['new_df'+str(i)].price.iloc[k] = globals()['new_df'+str(i)].price.iloc[k-1]\n",
    "                    p = k\n",
    "                    while globals()['new_df'+str(i)].price.iloc[k] == 0:\n",
    "                        globals()['new_df'+str(i)].price.iloc[k] = globals()['new_df'+str(i)].price.iloc[p]\n",
    "                        p = p +1\n",
    "\n",
    "\n",
    "        \n",
    "   \n",
    "    # making the final anc complete df with the prices of the index based on the weights   \n",
    "    final_df = new_df0\n",
    "    final_df.price = new_df0.price*weights[0]+new_df1.price*weights[1]+new_df2.price*weights[2]+new_df3.price*weights[3]+new_df4.price*weights[4]+new_df5.price*weights[5]+new_df6.price*weights[6]+new_df7.price*weights[7]+new_df8.price*weights[8]\n",
    "    \n",
    "    # computing the returns \n",
    "    final_df['ret'] = ((final_df.price - final_df.price.shift())/final_df.price.shift()).astype(float)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# group by Timestamp in seconds, in order to make it easier to compare with S&P500 als because I don't need all\n",
    "# these details in this phase, but I will in the specific analysis (look at quantities and market making and so on)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300a723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the df with the biggest interval possible, then I will select the small intervall I need:\n",
    "# (due to the fact that I want to consider a period of time as estimation window of 2 hours, I have to create 3 df\n",
    "# because each df can have max 1 hour of data (the API))    ===> then I will merge the dfs for the same news          \n",
    "# event at 18 GMT 1643220000000\n",
    "rates_hike_1 = pre_processing_CC('BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'LUNAUSDT', 'SOLUSDT', 'XRPUSDT', 'ADAUSDT', 'DOTUSDT', 'AVAXUSDT', 1643211000000, 1643212800000)\n",
    "rates_hike_2 = pre_processing_CC('BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'LUNAUSDT', 'SOLUSDT', 'XRPUSDT', 'ADAUSDT', 'DOTUSDT', 'AVAXUSDT', 1643212801000, 1643216400000)\n",
    "rates_hike_3 = pre_processing_CC('BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'LUNAUSDT', 'SOLUSDT', 'XRPUSDT', 'ADAUSDT', 'DOTUSDT', 'AVAXUSDT', 1643216401000, 1643220000000)\n",
    "rates_hike_4 = pre_processing_CC('BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'LUNAUSDT', 'SOLUSDT', 'XRPUSDT', 'ADAUSDT', 'DOTUSDT', 'AVAXUSDT', 1643220001000, 1643223600000)\n",
    "rates_hike_5 = pre_processing_CC('BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'LUNAUSDT', 'SOLUSDT', 'XRPUSDT', 'ADAUSDT', 'DOTUSDT', 'AVAXUSDT', 1643223601000, 1643227200000)\n",
    "\n",
    "# tapering_AR = pre_processing_CC('BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'LUNAUSDT', 'SOLUSDT', 1639591200000, 1639599000000)       \n",
    "# event at 18 GMT 1639591200000\n",
    "tapering_1 = pre_processing_CC('BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'LUNAUSDT', 'SOLUSDT', 'XRPUSDT', 'ADAUSDT', 'DOTUSDT', 'AVAXUSDT', 1639583000000, 1639584000000)\n",
    "tapering_2 = pre_processing_CC('BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'LUNAUSDT', 'SOLUSDT', 'XRPUSDT', 'ADAUSDT', 'DOTUSDT', 'AVAXUSDT', 1639584001000, 1639587600000)\n",
    "tapering_3 = pre_processing_CC('BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'LUNAUSDT', 'SOLUSDT', 'XRPUSDT', 'ADAUSDT', 'DOTUSDT', 'AVAXUSDT', 1639587601000, 1639591200000)\n",
    "tapering_4 = pre_processing_CC('BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'LUNAUSDT', 'SOLUSDT', 'XRPUSDT', 'ADAUSDT', 'DOTUSDT', 'AVAXUSDT', 1639591201000, 1639594800000)\n",
    "tapering_5 = pre_processing_CC('BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'LUNAUSDT', 'SOLUSDT', 'XRPUSDT', 'ADAUSDT', 'DOTUSDT', 'AVAXUSDT', 1639594801000, 1639598400000)\n",
    "\n",
    "# same for war:    1645660801000 = midnight and  sec 24-02 GMT. to 1645671600000 = 03 am 24-02 GMT \n",
    "# event at 2:20 GMT 1645669200000\n",
    "war_1 = pre_processing_CC('BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'LUNAUSDT', 'SOLUSDT', 'XRPUSDT', 'ADAUSDT', 'DOTUSDT', 'AVAXUSDT', 1645660000000, 1645660800000)\n",
    "war_2 = pre_processing_CC('BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'LUNAUSDT', 'SOLUSDT', 'XRPUSDT', 'ADAUSDT', 'DOTUSDT', 'AVAXUSDT', 1645660801000, 1645664400000)\n",
    "war_3 = pre_processing_CC('BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'LUNAUSDT', 'SOLUSDT', 'XRPUSDT', 'ADAUSDT', 'DOTUSDT', 'AVAXUSDT', 1645664401000, 1645668000000)\n",
    "war_4 = pre_processing_CC('BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'LUNAUSDT', 'SOLUSDT', 'XRPUSDT', 'ADAUSDT', 'DOTUSDT', 'AVAXUSDT', 1645668001000, 1645671600000)\n",
    "war_5 = pre_processing_CC('BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'LUNAUSDT', 'SOLUSDT', 'XRPUSDT', 'ADAUSDT', 'DOTUSDT', 'AVAXUSDT', 1645671600000, 1645675200000)\n",
    "\n",
    "\n",
    "# creating just 1 df for each news from which I can then select the subsamples and apply the functions\n",
    "rates_hike_1 = rates_hike_1.append(rates_hike_2)\n",
    "rates_hike_1 = rates_hike_1.append(rates_hike_3)\n",
    "rates_hike_1 = rates_hike_1.append(rates_hike_4)\n",
    "rates_hike_1 = rates_hike_1.append(rates_hike_5)\n",
    "rates_hike_1.ret.iloc[0] = 0.0\n",
    "rates_hike_1\n",
    "\n",
    "tapering_1 = tapering_1.append(tapering_2)\n",
    "tapering_1 = tapering_1.append(tapering_3)\n",
    "tapering_1 = tapering_1.append(tapering_4)\n",
    "tapering_1 = tapering_1.append(tapering_5)\n",
    "tapering_1.ret.iloc[0] = 0.0\n",
    "tapering_1\n",
    "\n",
    "war_1 = war_1.append(war_2)\n",
    "war_1 = war_1.append(war_3)\n",
    "war_1 = war_1.append(war_4)\n",
    "war_1 = war_1.append(war_5)\n",
    "war_1.ret.iloc[0] = 0.0\n",
    "war_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3095a5a",
   "metadata": {},
   "source": [
    "## Importing the already preprocessed data so that it will be easier and faster \n",
    "No need to run the previous part in this way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7071b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_hike_1 = pd.read_csv('rates_hike_CRIX.csv', index_col='Timestamp')\n",
    "tapering_1= pd.read_csv('tapering_CRIX.csv', index_col='Timestamp')\n",
    "war_1 = pd.read_csv('war_CRIX.csv', index_col='Timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ce445",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Abnormal Returns and T-Test for CRIX and general news (same news as before)</span> \n",
    "\n",
    "## computing AR, AAR and CAAR. the p-values for the AAR every 10 seconds after the event are computed according to the methodology used by Brown and Warner (1980) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1484430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the functions to compute the AR, AAR, CAR, CAAR and test the significance \n",
    "abnormal_ret_tappering_1 = abn_returns_10s_new(tapering_1.loc[1639583940:1639594440])\n",
    "abnormal_ret_rates_hike_1 = abn_returns_10s_new(rates_hike_1.loc[1643216340:1643223840]) \n",
    "abnormal_ret_war_1 = abn_returns_10s_new(war_1.loc[1645661940:1645669440])\n",
    "\n",
    "print(abnormal_ret_rates_hike_1, abnormal_ret_tappering_1, abnormal_ret_war_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96208af5",
   "metadata": {},
   "source": [
    "## Plot of cumulative returns in the small period of time as with S&P500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c3bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making small df for each news in order to plot them easily\n",
    "# for interest rtes\n",
    "plot_rate_cc = rates_hike_1.loc[1643223000:1643224800]\n",
    "plot_rate_cc['time_event'] = (plot_rate_cc.index-1643223600)/60\n",
    "plot_rate_cc = plot_rate_cc.set_index('time_event')\n",
    "\n",
    "# for tapering\n",
    "plot_tap_cc = tapering_1.loc[1639594200:1639596000]\n",
    "plot_tap_cc['time_event'] = (plot_tap_cc.index-1639594800)/60\n",
    "plot_tap_cc = plot_tap_cc.set_index('time_event')\n",
    "\n",
    "# for war\n",
    "plot_war_cc = war_1.loc[1645668600:1645670400]\n",
    "plot_war_cc['time_event'] = (plot_war_cc.index-1645669200)/60\n",
    "plot_war_cc = plot_war_cc.set_index('time_event')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94706583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tapering news impact:\n",
    "fig = plt.figure(figsize = (6,3))\n",
    "fig.suptitle('Tapering announcement Crypto') \n",
    "plt.plot(plot_tap_cc.ret.cumsum(), color= 'black')\n",
    "plt.axvline(x=0, linestyle='--', color= 'red')\n",
    "plt.xlabel('Minutes to the event')\n",
    "plt.ylabel(' Cumulative Returns')\n",
    "plt.savefig(\"plot_tapering_Crypto.jpg\")\n",
    "\n",
    "# rates hike\n",
    "fig = plt.figure(figsize = (6,3))\n",
    "fig.suptitle('Rates Hike announcement Crypto')\n",
    "plt.plot(plot_rate_cc.ret.cumsum(), color= 'black')     \n",
    "plt.axvline(x=0, linestyle='--', color= 'red')\n",
    "plt.xlabel('Minutes to the event')\n",
    "plt.ylabel(' Cumulative Returns')\n",
    "plt.savefig(\"plot_rates_hike_Crypto.jpg\")\n",
    "\n",
    "# War news impact:\n",
    "# starting at 1645578000000 (1 GMT and 3 Amsterdam time)\n",
    "# news from forbes arrived at 4.21 amsterdam time, so 2.21 GMT\n",
    "# finishing at 1645588800000 (1 GMT and 6 Amsterdam time)\n",
    "fig = plt.figure(figsize = (6,3))\n",
    "fig.suptitle('War announcement Crypto') \n",
    "plt.plot(plot_war_cc.ret.cumsum(), color= 'black')   \n",
    "plt.axvline(x=0, linestyle='--', color= 'red')  \n",
    "plt.xlabel('Minutes to the event')\n",
    "plt.ylabel(' Cumulative Returns')\n",
    "plt.savefig(\"plot_war_Crypto.jpg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041d2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of the CR of all the news of the two indexes\n",
    "# tapering news impact:\n",
    "fig, axs = plt.subplots(3, 2, figsize=(16, 13)) \n",
    "axs[0, 0].set_title('Tapering announcement')\n",
    "axs[0, 0].plot(plot_tap.ret.cumsum(), color= 'black')\n",
    "axs[0, 0].axvline(x=0, linestyle='--', color= 'red')\n",
    "\n",
    "# rates hike:\n",
    "axs[1, 0].set_title('Rates Hike announcement')  \n",
    "axs[1, 0].plot(plot_rate.ret.cumsum(), color= 'black')\n",
    "axs[1, 0].axvline(x=0, linestyle='--', color= 'red') \n",
    "\n",
    "# War news impact:\n",
    "axs[2, 0].set_title('War announcement')\n",
    "axs[2, 0].plot(plot_war.ret.cumsum(), color= 'black')\n",
    "axs[2, 0].axvline(x=0, linestyle='--', color= 'red')\n",
    "axs[2, 0].set(xlabel='Minutes to the event', ylabel='Cumulative Returns')\n",
    "\n",
    "\n",
    "# tapering news impact CC:\n",
    "axs[0, 1].set_title('Tapering announcement Crypto') \n",
    "axs[0, 1].plot(plot_tap_cc.ret.cumsum(), color= 'black')\n",
    "axs[0, 1].axvline(x=0, linestyle='--', color= 'red')\n",
    "\n",
    "\n",
    "# rates hike CC\n",
    "axs[1, 1].set_title('Rates Hike announcement Crypto')\n",
    "axs[1, 1].plot(plot_rate_cc.ret.cumsum(), color= 'black')     \n",
    "axs[1, 1].axvline(x=0, linestyle='--', color= 'red')\n",
    "\n",
    "\n",
    "# War news impact CC:\n",
    "axs[2, 1].set_title('War announcement Crypto') \n",
    "axs[2, 1].plot(plot_war_cc.ret.cumsum(), color= 'black')      \n",
    "axs[2, 1].axvline(x=0, linestyle='--', color= 'red')   \n",
    "axs[2, 1].set(xlabel='Minutes to the event', ylabel='Cumulative Returns')\n",
    "\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set( ylabel='Cumulative Returns') \n",
    "    \n",
    "    \n",
    "plt.savefig(\"cum_ret_all.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9700d970",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Specific Crypto analysis</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273be68a",
   "metadata": {},
   "source": [
    "### I  use BTC, ETH, ADA and XRP (that were also in the CRIX and has different projects and blockchain structure) + DOGE that is \"meme coin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b46e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create DF and modify the column names\n",
    "def data_crypto(crypto, event_time, cat):\n",
    "    # creating empty df that will be filled with the prices of the cryto\n",
    "    df_crypto = pd.DataFrame()\n",
    "    # getting the data from 3 hours before the event to 2 hours after\n",
    "    start_time = event_time - 10800000\n",
    "    for n in range (0,5):\n",
    "        end_time = start_time+3600000\n",
    "        response = requests.get('https://api.binance.com/api/v3/aggTrades?symbol=%(crypto)s&startTime=%(start_time)s&endTime=%(end_time)s' %locals())\n",
    "        df = response.json()\n",
    "        df = pd.DataFrame(df)\n",
    "        df = df.drop(columns=['a', 'm', 'M', 'f', 'l', 'q'])\n",
    "        df = df.rename(columns={'T': 'Timestamp', 'p':'price'}) #, 'q':'quantity'\n",
    "        \n",
    "        # changing from timestamp to seconds to the event\n",
    "        df.Timestamp = round((df.Timestamp-event_time), -3)/1000\n",
    "        df.price = df.price.astype(float)\n",
    "        df = df.groupby(by=['Timestamp']).mean()\n",
    "        # Adding the new dara to the previous df (of the same crypto)\n",
    "        df_crypto = df_crypto.append(df)\n",
    "        # continuing the rolling period of 1 hour\n",
    "        start_time = start_time +3600000 \n",
    "        \n",
    "    # aftert making the df with the prices of the crypto over the period, I compute the log returns\n",
    "    df_crypto['ret'] = np.log(df_crypto.price/df_crypto.price.shift())\n",
    "    \n",
    "    # then I compute the AR, CAR and I make the event study analysis as before, saving the excel file directly\n",
    "    avg_ret = df_crypto.loc[-7260:-60].ret.mean()\n",
    "    ar = pd.DataFrame(columns=['Seconds to the event', 'AAR', 'CAR', 'CAAR'], index = range(0,30))  # index = df.Date.iloc[600:]\n",
    "    n = -60\n",
    "    ar['significance'] = ''\n",
    "    ar['significance_CAR'] = ''\n",
    "    ar['significance_AR'] = ''\n",
    "    df_crypto['AR'] = df_crypto.ret - avg_ret\n",
    "\n",
    "    \n",
    "    # trying to compute the CAR in a meaningful way \n",
    "    df_crypto['CAR'] = 0\n",
    "    df_crypto.CAR.loc[-60:240] = df_crypto.loc[-60:240, 'AR'].cumsum()\n",
    "    \n",
    "    df_crypto = df_crypto.fillna(0)\n",
    "    AAR = df_crypto.loc[-7260:-60, 'AR'].mean() \n",
    "    \n",
    "\n",
    "    # considering the return every 10 seconds \n",
    "    for i in range(0, 30):\n",
    "        ar['Seconds to the event'].iloc[i] = n\n",
    "        \n",
    "    # AR is: observed ret-average\n",
    "    # I compute the Cumulative Abnormal Return for every 10 seconds\n",
    "        ar.CAR.iloc[i] = df_crypto.CAR.loc[n:n+10].mean()\n",
    " \n",
    "    # I compute the Average Abnormal Return for every 10 seconds\n",
    "        ar.AAR.iloc[i] = df_crypto.AR.loc[n:n+10].mean()\n",
    "        \n",
    "        n = n +10\n",
    "    # I compute the Cumulative Average Abnormal Return for every 10 seconds, summing the AAR    \n",
    "    ar.CAAR = ar.AAR.cumsum()\n",
    "    \n",
    "    # computing also the significance value of every AR and CAR for every second with the simple t-test\n",
    "    df_crypto['t_test_AR'] = 0.0001\n",
    "    df_crypto['t_test_AR'].loc[-60:] = (df_crypto.AR.loc[-60:]/np.std(df_crypto.AR.loc[-7260:-60])) \n",
    "    \n",
    "    # column to add the significance of the AR to the df\n",
    "    df_crypto['signif'] = 0.0\n",
    "\n",
    "    # adding the * to the respective significace level based on the t-values\n",
    "    df_crypto.signif.loc[df_crypto['t_test_AR'] >= 1.65] = '*'\n",
    "    df_crypto.signif.loc[df_crypto['t_test_AR'] <= -1.65] = '*'\n",
    "    df_crypto.signif.loc[df_crypto['t_test_AR'] >= 1.96] = '**'\n",
    "    df_crypto.signif.loc[df_crypto['t_test_AR'] <= -1.96] = '**'\n",
    "    df_crypto.signif.loc[df_crypto['t_test_AR'] >= 2.58] = '***'\n",
    "    df_crypto.signif.loc[df_crypto['t_test_AR'] <= -2.58] = '***'\n",
    "\n",
    "\n",
    "    \n",
    "    # formula to compute the p-value using the shapiro test for normality:\n",
    "    shapiro_test_norm = shapiro(ar.CAR).pvalue\n",
    "    print('Shapiro Test for normality: ' + str(shapiro_test_norm))\n",
    "    \n",
    "    if shapiro_test_norm > 0.05:  # normally distributed CAR \n",
    "        ar['t_test_CAR'] = ar.CAR/np.std(ar.CAR)\n",
    "        ar['t_test_AR'] = ar.AAR/np.std(ar.AAR)\n",
    "        \n",
    "        ar.significance_CAR.loc[ar['t_test_CAR'] >= 1.65] = '*'\n",
    "        ar.significance_CAR.loc[ar['t_test_CAR'] <= -1.65] = '*'\n",
    "        ar.significance_CAR.loc[ar['t_test_CAR'] >= 1.96] = '**'\n",
    "        ar.significance_CAR.loc[ar['t_test_CAR'] <= -1.96] = '**'\n",
    "        ar.significance_CAR.loc[ar['t_test_CAR'] >= 2.58] = '***'\n",
    "        ar.significance_CAR.loc[ar['t_test_CAR'] <= -2.58] = '***'\n",
    "        \n",
    "        ar.significance_AR.loc[ar['t_test_AR'] >= 1.65] = '*'\n",
    "        ar.significance_AR.loc[ar['t_test_AR'] <= -1.65] = '*'\n",
    "        ar.significance_AR.loc[ar['t_test_AR'] >= 1.96] = '**'\n",
    "        ar.significance_AR.loc[ar['t_test_AR'] <= -1.96] = '**'\n",
    "        ar.significance_AR.loc[ar['t_test_AR'] >= 2.58] = '***'\n",
    "        ar.significance_AR.loc[ar['t_test_AR'] <= -2.58] = '***'\n",
    "        print('Normal distributed CAR')\n",
    "        \n",
    "        \n",
    "    # sigma of AR in the estimation window\n",
    "    sigma = sqrt(sum((df_crypto.AR.loc[-7260:-60]-AAR)**2)/7200)\n",
    "\n",
    "    # computing t-statistics for AAR and CAAR\n",
    "    ar['t_test_AAR'] = ar.AAR/sigma\n",
    "    ar['t_test_CAAR'] = ar.CAAR/((301**0.5)*sigma)\n",
    "\n",
    "    ar['significance_CAAR'] = ''\n",
    "    ar.significance_CAAR.loc[ar['t_test_CAAR'] >= 1.65] = '*'\n",
    "    ar.significance_CAAR.loc[ar['t_test_CAAR'] <= -1.65] = '*'\n",
    "    ar.significance_CAAR.loc[ar['t_test_CAAR'] >= 1.96] = '**'\n",
    "    ar.significance_CAAR.loc[ar['t_test_CAAR'] <= -1.96] = '**'\n",
    "    ar.significance_CAAR.loc[ar['t_test_CAAR'] >= 2.58] = '***'\n",
    "    ar.significance_CAAR.loc[ar['t_test_CAAR'] <= -2.58] = '***'\n",
    "    \n",
    "    \n",
    "    ar.significance.loc[ar['t_test_AAR'] >= 1.65] = '*'\n",
    "    ar.significance.loc[ar['t_test_AAR'] <= -1.65] = '*'\n",
    "    ar.significance.loc[ar['t_test_AAR'] >= 1.96] = '**'\n",
    "    ar.significance.loc[ar['t_test_AAR'] <= -1.96] = '**'\n",
    "    ar.significance.loc[ar['t_test_AAR'] >= 2.58] = '***'\n",
    "    ar.significance.loc[ar['t_test_AAR'] <= -2.58] = '***'\n",
    "    # setting the degrees of freedom: in the reference paper are 120 (days of the estimation window - 1)\n",
    "    # so here are 7200 - 1 = 7199\n",
    "    dof = 7199\n",
    "    for i in range(0, len(ar.t_test_AAR)):\n",
    "        ar.t_test_AAR.iloc[i] = 2*(1 - t.cdf(abs(ar.t_test_AAR.iloc[i]), dof))\n",
    "       \n",
    "    \n",
    "    ar = ar.set_index('Seconds to the event')\n",
    "    df_crypto['CAR'] = df_crypto['AR'].cumsum()    \n",
    "        \n",
    "    # saving the file to excell in order to report the results (table) in the thesis document\n",
    "    ar.to_excel(str(crypto)+'_'+str(cat)+'.xlsx') \n",
    "\n",
    "    return df_crypto, ar\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d51e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the data and dividing into the categories. every result has in position 0 the FULL df with prices and AR, CAR (5 hours of data)\n",
    "# and the t-test on AR. in the position 1 of each result there are the AAR and CAAR over the 5 mins of the event window\n",
    "###### REGULATION:\n",
    "BTC_reg = data_crypto('BTCUSDT', 1632474001000, 'reg')\n",
    "ETH_reg = data_crypto('ETHUSDT', 1632474001000, 'reg')\n",
    "XRP_reg = data_crypto('XRPUSDT', 1608664380000, 'reg')\n",
    "DOGE_reg = data_crypto('DOGEUSDT', 1619635290000, 'reg')\n",
    "ADA_reg = data_crypto('ADAUSDT', 1637703363000, 'reg')\n",
    "\n",
    "###### PROJECTS and ADOPOTIONS\n",
    "BTC_proj = data_crypto('BTCUSDT', 1623218754000, 'proj')\n",
    "ETH_proj = data_crypto('ETHUSDT', 1653024783000, 'proj')\n",
    "XRP_proj = data_crypto('XRPUSDT', 1647854160000, 'proj')\n",
    "DOGE_proj = data_crypto('DOGEUSDT', 1639478063000, 'proj')\n",
    "ADA_proj = data_crypto('ADAUSDT', 1653494700000, 'proj')\n",
    "\n",
    "###### OPINION OF INFLUENT PEOPLE \n",
    "BTC_infl = data_crypto('BTCUSDT', 1622768824000, 'infl')      # around 4.5 mins it starts a significance movement \n",
    "ETH_infl = data_crypto('ETHUSDT', 1556586953000, 'infl')\n",
    "XRP_infl = data_crypto('XRPUSDT', 1632684902000, 'infl')     # price was already dropping and after the event, some minutes later, the trend inverted \n",
    "DOGE_infl = data_crypto('DOGEUSDT', 1618461198000, 'infl')    # super fast impact on the price\n",
    "ADA_infl = data_crypto('ADAUSDT', 1634325373000, 'infl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d2aec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the cumulative returns of all the specific cryptocurrencies for the cathegory Regulation \n",
    "# in the following order:\n",
    "# Bitcoin, Ethereum, Ripple, Dogecoion Cardano\n",
    "fig, axs = plt.subplots(5, 1, figsize=(8, 22))\n",
    "axs[0].set_title('BTC Regulation')\n",
    "axs[0].plot(BTC_reg[0].ret.loc[-60:240].cumsum(), color= 'black')\n",
    "axs[0].axvline(x=0, linestyle='--', color= 'red')\n",
    "\n",
    "\n",
    "axs[1].set_title('ETH Regulation')  \n",
    "axs[1].plot(ETH_reg[0].ret.loc[-60:240].cumsum(), color= 'black')\n",
    "axs[1].axvline(x=0, linestyle='--', color= 'red') # 1643227200\n",
    "\n",
    "\n",
    "axs[2].set_title('XRP Regulation')\n",
    "axs[2].plot(XRP_reg[0].ret.loc[-60:240].cumsum(), color= 'black')\n",
    "axs[2].axvline(x=0, linestyle='--', color= 'red')\n",
    "\n",
    "\n",
    "axs[3].set_title('DOGE Regulation')  \n",
    "axs[3].plot(DOGE_reg[0].ret.loc[-60:240].cumsum(), color= 'black')\n",
    "axs[3].axvline(x=0, linestyle='--', color= 'red') # 1643227200\n",
    "\n",
    "\n",
    "axs[4].set_title('ADA Regulation')\n",
    "axs[4].plot(ADA_reg[0].ret.loc[-60:240].cumsum(), color= 'black')\n",
    "axs[4].axvline(x=0, linestyle='--', color= 'red')\n",
    "axs[4].set(xlabel='Seconds to the event', ylabel='Cumulative Returns')\n",
    "\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set( ylabel='Cumulative Returns') \n",
    "    \n",
    "    \n",
    "plt.savefig(\"cum_ret_reg.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff606dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the cumulative returns of all the specific cryptocurrencies for the cathegory Projects/Adoption\n",
    "# in the following order:\n",
    "# Bitcoin, Ethereum, Ripple, Dogecoion Cardano\n",
    "fig, axs = plt.subplots(5, 1, figsize=(8, 22)) # , sharex=True)\n",
    "axs[0].set_title('BTC Project/Adoption')\n",
    "axs[0].plot(BTC_proj[0].ret.loc[-60:240].cumsum(), color= 'black')\n",
    "# plt.xticks(data_SP500.loc[1639597800:1639598700].index, data_SP500.loc[1639597800:1639598700].Date, rotation=90)\n",
    "axs[0].axvline(x=0, linestyle='--', color= 'red')\n",
    "\n",
    "\n",
    "axs[1].set_title('ETH Project/Adoption')  \n",
    "axs[1].plot(ETH_proj[0].ret.loc[-60:240].cumsum(), color= 'black')\n",
    "# plt.plot(data_SP500.loc[1643226600:1643228300].ret.cumsum(), color= 'black')\n",
    "# plt.xticks(data_SP500.loc[1639597800:1639598700].index, data_SP500.loc[1639597800:1639598700].Date, rotation=90)\n",
    "axs[1].axvline(x=0, linestyle='--', color= 'red') # 1643227200\n",
    "\n",
    "\n",
    "axs[2].set_title('XRP Project/Adoption')\n",
    "axs[2].plot(XRP_proj[0].ret.loc[-60:240].cumsum(), color= 'black')\n",
    "# plt.xticks(data_SP500.loc[1639597800:1639598700].index, data_SP500.loc[1639597800:1639598700].Date, rotation=90)\n",
    "axs[2].axvline(x=0, linestyle='--', color= 'red')\n",
    "\n",
    "\n",
    "axs[3].set_title('DOGE Project/Adoption')  \n",
    "axs[3].plot(DOGE_proj[0].ret.loc[-60:240].cumsum(), color= 'black')\n",
    "# plt.plot(data_SP500.loc[1643226600:1643228300].ret.cumsum(), color= 'black')\n",
    "# plt.xticks(data_SP500.loc[1639597800:1639598700].index, data_SP500.loc[1639597800:1639598700].Date, rotation=90)\n",
    "axs[3].axvline(x=0, linestyle='--', color= 'red') # 1643227200\n",
    "\n",
    "\n",
    "axs[4].set_title('ADA Project/Adoption')\n",
    "axs[4].plot(ADA_proj[0].ret.loc[-60:240].cumsum(), color= 'black')\n",
    "# plt.xticks(data_SP500.loc[1639597800:1639598700].index, data_SP500.loc[1639597800:1639598700].Date, rotation=90)\n",
    "axs[4].axvline(x=0, linestyle='--', color= 'red')\n",
    "axs[4].set(xlabel='Seconds to the event', ylabel='Cumulative Returns')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set( ylabel='Cumulative Returns') \n",
    "    \n",
    "    \n",
    "plt.savefig(\"cum_ret_proj.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd3fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the cumulative returns of all the specific cryptocurrencies for the cathegory Opinion of Influent people\n",
    "# in the following order:\n",
    "# Bitcoin, Ethereum, Ripple, Dogecoion Cardano\n",
    "fig, axs = plt.subplots(5, 1, figsize=(8, 22)) # , sharex=True)\n",
    "axs[0].set_title('BTC Project/Adoption')\n",
    "axs[0].plot(BTC_infl[0].ret.loc[-60:240].cumsum(), color= 'black')\n",
    "# plt.xticks(data_SP500.loc[1639597800:1639598700].index, data_SP500.loc[1639597800:1639598700].Date, rotation=90)\n",
    "axs[0].axvline(x=0, linestyle='--', color= 'red')\n",
    "\n",
    "\n",
    "axs[1].set_title('ETH Project/Adoption')  \n",
    "axs[1].plot(ETH_infl[0].ret.loc[-60:240].cumsum(), color= 'black')\n",
    "# plt.plot(data_SP500.loc[1643226600:1643228300].ret.cumsum(), color= 'black')\n",
    "# plt.xticks(data_SP500.loc[1639597800:1639598700].index, data_SP500.loc[1639597800:1639598700].Date, rotation=90)\n",
    "axs[1].axvline(x=0, linestyle='--', color= 'red') # 1643227200\n",
    "\n",
    "\n",
    "axs[2].set_title('XRP Project/Adoption')\n",
    "axs[2].plot(XRP_infl[0].ret.loc[-60:240].cumsum(), color= 'black')\n",
    "# plt.xticks(data_SP500.loc[1639597800:1639598700].index, data_SP500.loc[1639597800:1639598700].Date, rotation=90)\n",
    "axs[2].axvline(x=0, linestyle='--', color= 'red')\n",
    "\n",
    "\n",
    "axs[3].set_title('DOGE Project/Adoption')  \n",
    "axs[3].plot(DOGE_infl[0].ret.loc[-60:240].cumsum(), color= 'black')\n",
    "# plt.plot(data_SP500.loc[1643226600:1643228300].ret.cumsum(), color= 'black')\n",
    "# plt.xticks(data_SP500.loc[1639597800:1639598700].index, data_SP500.loc[1639597800:1639598700].Date, rotation=90)\n",
    "axs[3].axvline(x=0, linestyle='--', color= 'red') # 1643227200\n",
    "\n",
    "\n",
    "axs[4].set_title('ADA Project/Adoption')\n",
    "axs[4].plot(ADA_infl[0].ret.loc[-60:240].cumsum(), color= 'black')\n",
    "# plt.xticks(data_SP500.loc[1639597800:1639598700].index, data_SP500.loc[1639597800:1639598700].Date, rotation=90)\n",
    "axs[4].axvline(x=0, linestyle='--', color= 'red')\n",
    "axs[4].set(xlabel='Seconds to the event', ylabel='Cumulative Returns')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set( ylabel='Cumulative Returns') \n",
    "    \n",
    "    \n",
    "plt.savefig(\"cum_ret_infl.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae63a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results for the table of the comparison between the 3 categories\n",
    "###### REGULATION:\n",
    "regulation_average_AR = (BTC_reg[0].loc[4, 'AR'] + ETH_reg[0].loc[5, 'AR'] + DOGE_reg[0].loc[44, 'AR'] + ADA_reg[0].loc[5, 'AR'])/4\n",
    "regulation_average_CAR = (BTC_reg[0].loc[0:60, 'AR'].cumsum().mean()+ETH_reg[0].loc[0:60, 'AR'].cumsum().mean()+XRP_reg[0].loc[0:60, 'AR'].cumsum().mean()+DOGE_reg[0].loc[0:60, 'AR'].cumsum().mean()+ADA_reg[0].loc[0:60, 'AR'].cumsum().mean())/5\n",
    "regulation_average_CAR_10 = (BTC_reg[0].loc[0:600, 'AR'].cumsum().mean()+ETH_reg[0].loc[0:600, 'AR'].cumsum().mean()+XRP_reg[0].loc[0:600, 'AR'].cumsum().mean()+DOGE_reg[0].loc[0:600, 'AR'].cumsum().mean()+ADA_reg[0].loc[0:600, 'AR'].cumsum().mean())/5\n",
    "\n",
    "###### PROJECTS and ADOPOTIONS\n",
    "#dropping the first rows of BTC and DOGE because without values \n",
    "BTC_proj_clean = (BTC_proj[0].loc[0:60]).iloc[1:,:]\n",
    "DOGE_proj_clean = (DOGE_proj[0].loc[0:60]).iloc[1:,:]\n",
    "\n",
    "proj_average_AR = (XRP_proj[0].loc[22, 'AR']+ DOGE_proj_clean.loc[0, 'AR'] + ADA_proj[0].loc[42, 'AR'])/3\n",
    "proj_average_CAR = (BTC_proj_clean.loc[0:60, 'AR'].cumsum().mean()+ETH_proj[0].loc[0:60, 'AR'].cumsum().mean()+XRP_proj[0].loc[0:60, 'AR'].cumsum().mean()+DOGE_proj_clean.loc[0:60, 'AR'].cumsum().mean()+ADA_proj[0].loc[0:60, 'AR'].cumsum().mean())/5\n",
    "proj_average_CAR_10 = (BTC_proj[0].loc[0:600, 'AR'].cumsum().mean()+ETH_proj[0].loc[0:600, 'AR'].cumsum().mean()+XRP_proj[0].loc[0:600, 'AR'].cumsum().mean()+DOGE_proj_clean.loc[0:60, 'AR'].cumsum().mean()+ADA_proj[0].loc[0:600, 'AR'].cumsum().mean())/5\n",
    "\n",
    "###### OPINION OF INFLUENT PEOPLE\n",
    "#dropping the first rows of BTC and DOGE because without values \n",
    "BTC_infl_clean = (BTC_infl[0].loc[0:60]).iloc[1:,:]\n",
    "DOGE_infl_clean = (DOGE_infl[0].loc[0:60]).iloc[1:,:]\n",
    "\n",
    "infl_average_AR = (BTC_infl_clean.loc[8, 'AR'] + ETH_infl[0].loc[24, 'AR'] + DOGE_infl_clean.loc[2, 'AR'] + XRP_infl[0].loc[4, 'AR'])/4\n",
    "infl_average_CAR = (BTC_infl_clean.loc[0:60, 'AR'].cumsum().mean()+ETH_infl[0].loc[0:60, 'AR'].cumsum().mean()+XRP_infl[0].loc[0:60, 'AR'].cumsum().mean()+DOGE_infl_clean.loc[0:60, 'AR'].cumsum().mean()+ADA_infl[0].loc[0:60, 'AR'].cumsum().mean())/5\n",
    "infl_average_CAR_10 = (BTC_infl[0].loc[0:600, 'AR'].cumsum().mean()+ETH_infl[0].loc[0:600, 'AR'].cumsum().mean()+XRP_infl[0].loc[0:600, 'AR'].cumsum().mean()+DOGE_infl[0].loc[0:600, 'AR'].cumsum().mean()+ADA_infl[0].loc[0:600, 'AR'].cumsum().mean())/5\n",
    "\n",
    "\n",
    "print(regulation_average_AR, regulation_average_CAR, regulation_average_CAR_10) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
